{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process\n",
    "\n",
    "1. 데이터 경로 정하기\n",
    "2. base 디렉토리 설정\n",
    "3. 모듈 임포트\n",
    "4. 추론\n",
    "  1. 모델 로드\n",
    "  2. 모델 메모리 적재 (변수에)\n",
    "  3. **오디오 파일 받을지(예상)**, request로 들어온 오디오 자체 사용할지\n",
    "  4. 오디오 파일 경로\n",
    "  5. 모델에 입력 -> 아웃풋 get_likely_index 입력하여 -> 우리가 원하는 라벨(텍스트) 값으로 변경하여 반환\n",
    "  6. 클라이언트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import soundfile\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/home/team2/workspace'\n",
    "\n",
    "# 서버 경로\n",
    "ZIP_BASE_DIR = '/data/team2/audio/'\n",
    "EXTRACT_BASE_DIR = ZIP_BASE_DIR + 'Training/data/remote/PROJECT/AI학습데이터/KoreanSpeech/data'\n",
    "\n",
    "# /media/{case_pk}/{audio_pk}.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_standard_length = 160000\n",
    "\n",
    "def get_speech(file_path):\n",
    "    waveform, sample_rate = torchaudio.backend.soundfile_backend.load(file_path)\n",
    "\n",
    "    length = waveform.size(1)\n",
    "    result = torch.zeros((1, audio_standard_length))\n",
    "    idx = (audio_standard_length - waveform.size(1)) // 2\n",
    "\n",
    "    result[0, idx:idx+length] = waveform\n",
    "\n",
    "    return result, sample_rate \n",
    "\n",
    "def make_melspectogram(file_path):\n",
    "    waveform, sample_rate = get_speech(file_path)\n",
    "\n",
    "    n_fft = 512\n",
    "    win_length = 512\n",
    "    hop_length = 256\n",
    "    n_mels = 128\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    melspec = mel_spectrogram(waveform)             # 결과값\n",
    "\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 확인용 데이터\n",
    "inference_data = []\n",
    "with open('../audio_data_test', 'r') as f:\n",
    "    inference_data = list(map(lambda x: x.split('\\t')[0], f.readlines()))[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team2/anaconda3/lib/python3.8/site-packages/torchaudio/functional/functional.py:507: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mel_specs = tuple(map(lambda x: make_melspectogram(EXTRACT_BASE_DIR + x), inference_data))\n",
    "\n",
    "mel_specs_dataset = torch.stack(mel_specs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMobilenetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomMobilenetV2, self).__init__()\n",
    "        mobilenet = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,1)),\n",
    "            *list(mobilenet.features)[:-1])\n",
    "        self.classifier = nn.Linear(1280*20, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).view(-1, 320*4*20)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/team2/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = CustomMobilenetV2(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model):\n",
    "    model.load_state_dict(torch.load('../model_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4972, 0.9863, 1.3125, 0.7184, 1.8279, 0.6440],\n",
      "        [1.5952, 1.0205, 1.1982, 0.8567, 1.9987, 0.6770],\n",
      "        [1.6216, 0.8704, 1.1452, 1.1122, 2.1213, 0.3271],\n",
      "        [1.5131, 0.9181, 1.1979, 0.8749, 2.1032, 0.7557]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(mel_specs_dataset)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산된 결과 (1x6)에서 가장 큰 확률이 predict label\n",
    "def get_likely_index(tensor):\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "result_label = get_likely_index(output)\n",
    "print(result_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['전라', '전라', '전라', '전라']\n"
     ]
    }
   ],
   "source": [
    "label_dict = {0: '서울,경기', 1: '강원', 2: '충청', 3: '경상', 4: '전라', 5: '제주'}\n",
    "\n",
    "result = list(map(lambda x: label_dict[x], result_label.tolist()))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e59c7e84344f2f535a0e17e840b5691184aa424755b7e18639b35c189d264181"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
