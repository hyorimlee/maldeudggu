{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지역방언 음성 분류 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "- 압축 풀기\n",
    "- 라벨 데이터 처리\n",
    "- 라벨 별 오디오 파일 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules, Configs\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# 압축 파일 경로\n",
    "ZIP_BASE_DIR = 'D:/ssafy_ai/한국인 대화 음성/'\n",
    "\n",
    "# 라벨 데이터 audio_path 앞에 붙는 경로\n",
    "EXTRACT_BASE_DIR = './Training/data/remote/PROJECT/AI학습데이터/KoreanSpeech/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zips = os.listdir(ZIP_BASE_DIR + 'Training')\n",
    "valid_zips = os.listdir(ZIP_BASE_DIR + 'Validation')\n",
    "\n",
    "train_label_zips = filter(lambda x: '[라벨]' in x, train_zips)\n",
    "\n",
    "for train_label_zip in train_label_zips:\n",
    "    train_audio_zip = train_label_zip.replace('[라벨]', '[원천]')\n",
    "\n",
    "    # label_tar = tarfile.open(ZIP_BASE_DIR + 'Training/' + train_label_zip)\n",
    "    # label_tar.extractall('./Training')\n",
    "    # label_tar.close()\n",
    "\n",
    "    # audio_tar = tarfile.open(ZIP_BASE_DIR + 'Training/' + train_audio_zip)\n",
    "    # audio_tar.extractall('./Training')\n",
    "    # audio_tar.close()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOB_PATH = './Training/**/*.txt'\n",
    "\n",
    "file_list = glob(GLOB_PATH, recursive=True)\n",
    "metadata_list = filter(lambda x: 'metadata' in x, file_list)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for metadata in metadata_list:\n",
    "    with open(metadata, 'r', encoding='UTF-8') as f:\n",
    "        for l in f.readlines():\n",
    "            try:\n",
    "                data = l.split('|')\n",
    "\n",
    "                audio_path = data[0].rstrip()               # 음성 파일 경로    \n",
    "                dialect = int(data[6])                      # 라벨링 데이터 - (1: 서울,경기, 2: 강원, 3: 충청, 4: 경상, 5: 전라, 6: 제주, 9: 기타)\n",
    "\n",
    "                labels.append((audio_path, dialect))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counter = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0,\n",
    "    5: 0, \n",
    "    6: 0,\n",
    "    9: 0\n",
    "}\n",
    "for _, label in labels:\n",
    "    label_counter[label]+=1\n",
    "\n",
    "MIN_AUDIO_LENGTH = min(list(label_counter.values())[:-1])\n",
    "MIN_AUDIO_LENGTH\n",
    "\n",
    "print(label_counter)\n",
    "print(MIN_AUDIO_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셔플 (임시)\n",
    "from random import shuffle\n",
    "shuffle(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 처리\n",
    "- 데이터를 20개씩 라벨별로 분리\n",
    "- Train, Test Dataset 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules, Configs\n",
    "from copy import deepcopy\n",
    "\n",
    "# 라벨 데이터를 20개씩 균등하게 분배\n",
    "MAX_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 20개씩 분리한 리스트로 저장\n",
    "label_source = {i:[] for i in range(1, 7)}\n",
    "label_dataset = [deepcopy(label_source) for _ in range(MIN_AUDIO_LENGTH//MAX_LENGTH)]\n",
    "label_idxs = {i:0 for i in range(1, 7)}\n",
    "for audio_path, label in labels:\n",
    "    if label == 9:\n",
    "        continue\n",
    "    idx = label_idxs[label]\n",
    "    try:\n",
    "        label_dataset[idx][label].append(audio_path)\n",
    "        if len(label_dataset[idx][label]) >= MAX_LENGTH:\n",
    "            label_idxs[label] +=1\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 라벨별로 10000개씩만 가져와서 학습시켜보기 (Train : 60000, Test : 60000)\n",
    "temp_dataset = []\n",
    "for i in range(500):\n",
    "    for label in label_dataset[i]:\n",
    "        for audio_path in label_dataset[i][label]:\n",
    "            temp_dataset.append(audio_path+'\\t'+str(label))\n",
    "\n",
    "# train용 오디오 데이터 경로 + 라벨 임시 저장\n",
    "with open('./audio_data_train', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(temp_dataset))\n",
    "\n",
    "    \n",
    "temp_dataset = []\n",
    "for i in range(500,1000):\n",
    "    for label in label_dataset[i]:\n",
    "        for audio_path in label_dataset[i][label]:\n",
    "            temp_dataset.append(audio_path+'\\t'+str(label))\n",
    "\n",
    "# test용 오디오데이터 경로 + 라벨 임시 저장\n",
    "with open('./audio_data_test', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(temp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리에 필요한 zeropadding, cutting을 위해 audio_length 체크하는 부분, 로컬에서 엄청오래걸리길래 서버에 돌려놓음\n",
    "audio_length = []\n",
    "for audio_path, _ in labels:\n",
    "    try:\n",
    "        waveform, sample_rate = get_speech(EXTRACT_BASE_DIR + audio_path.replace('06.경제', '6.경제'))\n",
    "        audio_length.append(waveform.size()[-1])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "- Pytorch Dataset, Dataloader 정의\n",
    "- Mobilenet v2\n",
    "- 모델 커스터마이징\n",
    "- 학습, 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules, Configs, Utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# !pip install SoundFile\n",
    "import soundfile\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 클래스 개수\n",
    "NUM_CLASSES = 6\n",
    "torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False\n",
    "\n",
    "# 음성 데이터 불러오기\n",
    "def get_speech(file_path):\n",
    "    return torchaudio.backend.soundfile_backend.load(file_path)\n",
    "\n",
    "n_fft = 512\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "n_mels = 128\n",
    "\n",
    "# Melspectogram 변환\n",
    "def make_melspectogram(audio_path):\n",
    "    waveform, sample_rate = get_speech(audio_path)\n",
    "    \n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    melspec = mel_spectrogram(waveform)             # 결과값\n",
    "\n",
    "    # print(melspec)\n",
    "    # print(melspec.size())\n",
    "    # f[audio_path.split('/')[-1]] = melspec.numpy()\n",
    "\n",
    "    return True, melspec.numpy()\n",
    "\n",
    "    # except RuntimeError as re:\n",
    "    #     if str(re.args[0]).startswith('Error opening'):\n",
    "    #         print('**********  파일 열기 에러 (경로 및 이름 확인 필요)  **********')\n",
    "    #         print(f'오류 발생한 경로 : {EXTRACT_BASE_DIR}{audio_path}')\n",
    "    # except:\n",
    "    #     print('melspectrogram 변환 과정 중 오류 발생')\n",
    "\n",
    "    # return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0+cpu'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 버전체크 (서버에서는 0.11.0 cuda 사용)\n",
    "torchaudio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialectAudioDataset(Dataset):\n",
    "    def __init__(self, label_file, root_dir, transform=None, target_transform=None):\n",
    "        with open(label_file, 'r', encoding='UTF-8') as f:\n",
    "            self.audio_datas = [l.rstrip().split('\\t') for l in f.readlines()]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.root_dir + self.audio_datas[idx][0]\n",
    "        # audio melspectogram\n",
    "        # image = read_image(img_path)\n",
    "        # audio = get_speech(self.root_dir, self.audio_datas[idx][0])\n",
    "        \n",
    "        # print(self.audio_datas[idx])\n",
    "        \n",
    "        # 경로상 오류있음, 처리\n",
    "        audio_path = audio_path.replace('06.경제', '6.경제')\n",
    "\n",
    "        # 데이터 로드 시 melspec처리까지 진행, 확인했을 때 25정도까지도 줄어들었음\n",
    "        audio = make_melspectogram(audio_path)[1][:,:,:20]\n",
    "        \n",
    "        # label은 0부터 시작\n",
    "        label = torch.tensor([int(self.audio_datas[idx][1]) - 1], dtype=torch.long)\n",
    "\n",
    "        # zero padding, noise padding 필요\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "training_data = DialectAudioDataset('./audio_data_train', EXTRACT_BASE_DIR)\n",
    "testing_data = DialectAudioDataset('./audio_data_test', EXTRACT_BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testing_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sosin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pillow\n",
    "mobilenet = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): ConvBNActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모바일넷의 구조를 확인가능\n",
    "print(mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리 데이터의 경우 1 channel, 128 x audio_length의 구조를 가지고있음\n",
    "# 모바일넷의 경우 in_channel이 3으로 시작 (vision model의 특성, rgb 3차원)\n",
    "# 따라서, 모델의 in channel을 1로 받아서 3으로 늘려주는 conv net 을 하나 추가\n",
    "\n",
    "# 모바일넷 classification의 classifier는 1000개 (이미지 1000개를 분류하는 대회용으로 만들어진 모델이라 그럼)\n",
    "# 1000개 분류 모델이 아닌 6개(수도권,전라,충청,제주,경상,강원) 분류 모델이므로 classifier 부분을 재정의\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, originalModel, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # in channel 1->3 추가,  classifier 재정의\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,1)), *list(originalModel.features)[:-1])\n",
    "        self.classifier = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).view(-1, 320*4)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = CustomModel(mobilenet, NUM_CLASSES)\n",
    "\n",
    "# Loss함수 cross entropy loss\n",
    "creterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습, 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules, Configs\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# !pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 학습 디바이스 체크\n",
    "# GPU서버에선 cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습함수\n",
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        # data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # cross_entropy loss (batch * NUM_CLASSES, batch * 1) [[0.13,0.13,0.13,0.13,0.13,0.35], ...] , [6, ...]\n",
    "        loss = creterion(output.squeeze(), target.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춘 개수\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "# 계산된 결과 (1x6)에서 가장 큰 확률이 predict label\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        # data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0005330490405117271/2 [00:02<3:02:53, 5488.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.051190\n",
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0010660980810234541/2 [00:05<3:02:52, 5489.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0015991471215351812/2 [00:08<2:57:03, 5315.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0021321961620469083/2 [00:11<2:55:02, 5256.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0026652452025586353/2 [00:14<2:53:00, 5197.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0031982942430703624/2 [00:16<2:54:39, 5247.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0037313432835820895/2 [00:19<2:55:22, 5271.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0042643923240938165/2 [00:22<2:55:18, 5270.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.004797441364605543/2 [00:25<2:53:47, 5226.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00533049040511727/2 [00:28<2:55:58, 5293.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.0058635394456289965/2 [00:31<2:58:17, 5364.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.006396588486140723/2 [00:33<2:53:59, 5236.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00692963752665245/2 [00:36<2:52:44, 5200.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.007462686567164176/2 [00:39<2:53:43, 5231.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.007995735607675903/2 [00:42<2:51:54, 5178.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00852878464818763/2 [00:44<2:54:30, 5257.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.009061833688699356/2 [00:47<2:56:09, 5308.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.009594882729211083/2 [00:50<2:56:22, 5316.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 0.01012793176972281/2 [00:53<2:55:07, 5280.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 0.010660980810234536/2 [00:56<2:58:42, 5389.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 0.011194029850746263/2 [00:59<3:00:47, 5454.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.064970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 0.011194029850746263/2 [00:59<2:57:34, 5357.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ssafy_ai\\test2.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000029?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mn_epoch) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000029?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000029?line=8'>9</a>\u001b[0m         train(model, epoch, log_interval)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000029?line=9'>10</a>\u001b[0m         test(model, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000029?line=10'>11</a>\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32md:\\ssafy_ai\\test2.ipynb Cell 27'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoch, log_interval)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000026?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, epoch, log_interval):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000026?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000026?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000026?line=5'>6</a>\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000026?line=6'>7</a>\u001b[0m         target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32md:\\ssafy_ai\\test2.ipynb Cell 14'\u001b[0m in \u001b[0;36mDialectAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=17'>18</a>\u001b[0m \u001b[39m# audio melspectogram\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=18'>19</a>\u001b[0m \u001b[39m# image = read_image(img_path)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=19'>20</a>\u001b[0m \u001b[39m# audio = get_speech(self.root_dir, self.audio_datas[idx][0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=21'>22</a>\u001b[0m \u001b[39m# print(self.audio_datas[idx])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=22'>23</a>\u001b[0m audio_path \u001b[39m=\u001b[39m audio_path\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m06.경제\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m6.경제\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=23'>24</a>\u001b[0m audio \u001b[39m=\u001b[39m make_melspectogram(audio_path)[\u001b[39m1\u001b[39m][:,:,:\u001b[39m20\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=25'>26</a>\u001b[0m \u001b[39m# label은 0부터 시작\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000015?line=26'>27</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_datas[idx][\u001b[39m1\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "\u001b[1;32md:\\ssafy_ai\\test2.ipynb Cell 11'\u001b[0m in \u001b[0;36mmake_melspectogram\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_melspectogram\u001b[39m(audio_path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=19'>20</a>\u001b[0m     waveform, sample_rate \u001b[39m=\u001b[39m get_speech(audio_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=21'>22</a>\u001b[0m     mel_spectrogram \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mMelSpectrogram(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=22'>23</a>\u001b[0m         sample_rate\u001b[39m=\u001b[39msample_rate,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=23'>24</a>\u001b[0m         n_fft\u001b[39m=\u001b[39mn_fft,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=32'>33</a>\u001b[0m         mel_scale\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhtk\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=33'>34</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=35'>36</a>\u001b[0m     melspec \u001b[39m=\u001b[39m mel_spectrogram(waveform)             \u001b[39m# 결과값\u001b[39;00m\n",
      "\u001b[1;32md:\\ssafy_ai\\test2.ipynb Cell 11'\u001b[0m in \u001b[0;36mget_speech\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_speech\u001b[39m(file_path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ssafy_ai/test2.ipynb#ch0000011?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torchaudio\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49msoundfile_backend\u001b[39m.\u001b[39;49mload(file_path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\torchaudio\\backend\\soundfile_backend.py:197\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=122'>123</a>\u001b[0m \u001b[39m@_mod_utils\u001b[39m\u001b[39m.\u001b[39mrequires_soundfile()\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=123'>124</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=124'>125</a>\u001b[0m     filepath: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=129'>130</a>\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=130'>131</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=131'>132</a>\u001b[0m     \u001b[39m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=132'>133</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=133'>134</a>\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=194'>195</a>\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=195'>196</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=196'>197</a>\u001b[0m     \u001b[39mwith\u001b[39;00m soundfile\u001b[39m.\u001b[39;49mSoundFile(filepath, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file_:\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=197'>198</a>\u001b[0m         \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mformat \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWAV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m normalize:\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/torchaudio/backend/soundfile_backend.py?line=198'>199</a>\u001b[0m             dtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\soundfile.py:629\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=625'>626</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=626'>627</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=627'>628</a>\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=628'>629</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=629'>630</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=630'>631</a>\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=631'>632</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssafy\\lib\\site-packages\\soundfile.py:1175\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=1172'>1173</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=1173'>1174</a>\u001b[0m             file \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mencode(_sys\u001b[39m.\u001b[39mgetfilesystemencoding())\n\u001b[1;32m-> <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=1174'>1175</a>\u001b[0m     file_ptr \u001b[39m=\u001b[39m openfunction(file, mode_int, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info)\n\u001b[0;32m   <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=1175'>1176</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(file, \u001b[39mint\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/sosin/anaconda3/envs/ssafy/lib/site-packages/soundfile.py?line=1176'>1177</a>\u001b[0m     file_ptr \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_open_fd(file, mode_int, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info, closefd)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 2\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c493a5e115826207e1bb90626615dcfbd1365e511670cda6109ac4335ea24144"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
